# Zyrabit LLM Secure Suite
[![English](https://img.shields.io/badge/lang-English-blue.svg)](README_EN.md)
![Python](https://img.shields.io/badge/python-v3.10+-blue.svg)
![Docker](https://img.shields.io/badge/docker--compose-ready-green.svg)
![License](https://img.shields.io/badge/license-MIT-yellow.svg)
![Architecture](https://img.shields.io/badge/architecture-clean-orange.svg)

**Zyrabit LLM Secure Suite** es una arquitectura de referencia para desplegar agentes de IA generativa seguros y privados en entornos empresariales. Combina la potencia de **Ollama (Phi-3)** con una capa de seguridad intermedia que sanitiza datos sensibles antes de que toquen el LLM.

## Arquitectura

El proyecto se divide en dos componentes principales:

1.  **Frontend (Ra√≠z)**:
    *   `app.py`: Dashboard de Streamlit para interacci√≥n con el usuario.
    *   `secure_agent.py`: Agente CLI para pruebas r√°pidas y seguras.
2.  **Backend (`zyrabit-brain-api`)**:
    *   `api-rag/`: API FastAPI que orquesta la l√≥gica de RAG, conecta con ChromaDB y Ollama.

```mermaid
graph TD
    subgraph "Frontend"
        User((üë§ Usuario))
        UI["ÔøΩÔ∏è Streamlit (app.py)"]
        CLI["ÔøΩ CLI (secure_agent.py)"]
    end

    subgraph "Backend (Docker Network)"
        API["‚ö° FastAPI (api-rag)"]
        LLM["üß† Ollama (Phi-3)"]
        Embed["üß© Ollama (mxbai-embed-large)"]
        VectorDB[("ÔøΩÔ∏è ChromaDB")]
    end

    User --> UI
    User --> CLI
    UI -->|HTTP POST /v1/chat| API
    CLI -->|HTTP POST /v1/chat| API
    API -->|Generar Embeddings| Embed
    API -->|Buscar Contexto| VectorDB
    API -->|Generar Respuesta| LLM
```

## Propuesta de Valor

1.  **Privacidad por Dise√±o**: Ning√∫n dato PII (Emails, Tel√©fonos, Tarjetas de Cr√©dito) llega al modelo de lenguaje. El agente seguro act√∫a como un firewall de datos.
2.  **Soberan√≠a de Datos**: Ejecuci√≥n 100% local u on-premise utilizando modelos eficientes como Phi-3.
3.  **Observabilidad Completa**: Stack de monitoreo integrado para trazar latencia, uso de tokens y errores en tiempo real.
4.  **Arquitectura Modular**: Componentes desacoplados (Cliente, API, LLM, VectorDB) que permiten escalar independientemente.

## Instalaci√≥n

### Prerrequisitos
*   Docker & Docker Compose
*   Python 3.10+
*   Ollama (para ejecuci√≥n local sin Docker)

### Pasos R√°pidos

1.  **Clonar el repositorio**:
    ```bash
    git clone https://github.com/Zyrabit-tech/zyrabit-llm.git
    cd zyrabit-llm
    ```

2.  **Configurar Entorno**:
    ```bash
    # Instalar dependencias de Python
    pip install -r requirements.txt
    
    # Configurar Ollama y descargar modelos (phi3 + mxbai-embed-large)
    chmod +x setup_ollama.sh
    ./setup_ollama.sh
    ```

3.  **Ingesta de Documentos (RAG)**:
    Para alimentar la memoria vectorial con tus propios documentos, utiliza el endpoint de la API:

    **Opci√≥n A: V√≠a cURL**
    ```bash
    curl -X POST "http://localhost:8080/v1/ingest" \
         -H "accept: application/json" \
         -H "Content-Type: multipart/form-data" \
         -F "file=@/ruta/a/tu/documento.pdf"
    ```

    **Opci√≥n B: V√≠a Interfaz Swagger**
    1.  Abre `http://localhost:8080/docs` en tu navegador.
    2.  Busca el endpoint `POST /v1/ingest`.
    3.  Sube tu archivo PDF (M√°x 800MB).

    El sistema procesar√° el PDF, generar√° embeddings con `mxbai-embed-large` y los guardar√° en ChromaDB autom√°ticamente.

4.  **Ejecutar Agente Seguro**:
    ```bash
    python3 secure_agent.py
    ```

## Contribuci√≥n

¬°Queremos tu ayuda para hacer Zyrabit LLM mejor!
Por favor lee nuestras [Gu√≠as de Contribuci√≥n](CONTRIBUTING.md) para conocer nuestro flujo de trabajo, convenci√≥n de commits y c√≥mo empezar.

**Recuerda**: Los Pull Requests deben apuntar a la rama `beta`.

## Troubleshooting

*   **Error de conexi√≥n con Ollama**: Aseg√∫rate de que Ollama est√© corriendo (`ollama serve`) y escuchando en el puerto 11434.
*   **Modelo no encontrado**: Ejecuta `./setup_ollama.sh` para asegurar que `phi3` y `mxbai-embed-large` est√©n descargados.
*   **Permisos de ejecuci√≥n**: Si `setup_ollama.sh` falla, aseg√∫rate de haber ejecutado `chmod +x setup_ollama.sh`.

______________________

# Run The App chat

```bash 
streamlit run app.py
```