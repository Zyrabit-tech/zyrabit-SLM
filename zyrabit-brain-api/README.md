RAG-Stack-Local üöÄ

Tu propio "cerebro" de IA, 100% open-sources

Este proyecto te da el stack completo para correr un sistema de Retrieval-Augmented Generation (RAG) en tu propia m√°quina o servidor. Olv√≠date de APIs de terceros, facturas impredecibles y de mandar tus datos sensibles a la nube de alguien m√°s.

Aqu√≠, t√∫ tienes el control.

üß† La Filosof√≠a: Por Qu√© Construimos Esto

Vivimos una revoluci√≥n de IA, pero la mayor√≠a de las soluciones nos piden sacrificar el control por la conveniencia. Este proyecto se basa en 4 principios:

Control Total (Self-Hosted): Tus modelos, tus datos, tus reglas. Todo corre en tu hardware (local o en la nube que t√∫ elijas). No m√°s dependencia de APIs externas.

Arquitectura Desacoplada: Inspirado en microservicios. El "Cerebro" (API), el "M√∫sculo" (LLM) y la "Memoria" (VectorDB) son contenedores separados. ¬øNecesitas escalar la GPU? Escala solo el "M√∫sculo". ¬øQuieres cambiar de Chroma a Qdrant? Cambia solo la "Memoria".

Portabilidad (Docker): docker compose up. Funciona en el Mac M2 de tu dev, funciona en un Droplet de DigitalOcean con GPU, y funciona en tu server bare-metal. Sin drama.

Observabilidad Primero: No volamos a ciegas. El stack incluye Prometheus y Grafana desde el d√≠a cero. Si es lento, sabr√°s por qu√© es lento (CPU, VRAM, I/O...).

üèóÔ∏è Arquitectura del Stack

Este es el plan. Usamos docker-compose para orquestar 5 servicios que se hablan entre s√≠ en una red interna.

graph TD
    subgraph "Tu M√°quina (Host/Server)"
        User[Dev/Usuario]
        IngestScript[ingest.py (Script de Carga)]
        SourceDocs[PDFs/Libros/Docs]
        Gpu[Host GPU]
        VolModelos[Volumen: ./ollama-models]
        VolVectores[Volumen: ./chroma-data]
    end

    subgraph "Stack Docker (docker-compose)"
        ApiRag(api-rag: FastAPI<br/>El Cerebro<br/>Port: 8080)
        LLM(llm-server: Ollama<br/>El M√∫sculo)
        DB(vector-db: Chroma<br/>La Memoria)
        Prom(prometheus: Monitor<br/>Port: 9090)
        Graf(grafana: Dashboard<br/>Port: 3000)
    end

    %% --- Flujos de Interacci√≥n ---
    User -- HTTP Request --> ApiRag
    ApiRag -- 1. Busca contexto --> DB
    DB -- 2. Devuelve chunks --> ApiRag
    ApiRag -- 3. Prompt aumentado --> LLM
    LLM -- 4. Genera respuesta --> ApiRag
    ApiRag -- 5. Respuesta final --> User

    %% --- Flujo de Ingesta (Data Ingestion)
    SourceDocs -- 1. Lee --> IngestScript
    IngestScript -- 2. Vectoriza y Escribe --> DB

    %% --- Dependencias de Hardware y Datos
    Gpu -- Acelera --> LLM
    VolModelos -- Monta modelos --> LLM
    VolVectores -- Persiste vectores --> DB

    %% --- Flujo de Monitoreo
    Prom -- Scrape metrics (scrapea) --> ApiRag
    Graf -- Visualiza datos de --> Prom
    User -- Revisa dashboards --> Graf(http://localhost:3000)


üõ†Ô∏è Tech Stack

API (Cerebro): FastAPI (Python)

Servidor LLM (M√∫sculo): Ollama

VectorDB (Memoria): ChromaDB

Orquestaci√≥n: Docker-Compose

Monitoreo: Prometheus & Grafana

üöÄ C√≥mo Empezar

Prerrequisitos

Docker

Docker Compose

git

(Opcional, pero recomendado) Un host con GPU NVIDIA y los drivers de NVIDIA + NVIDIA Container Toolkit.

Instalaci√≥n

Clona el repositorio:

git clone [https://github.com/TU_USUARIO/TU_REPO.git](https://github.com/TU_USUARIO/TU_REPO.git)
cd TU_REPO


Crea los directorios para los vol√∫menes:
(Esto evita problemas de permisos con Docker)

mkdir -p ollama-models
mkdir -p chroma-data


Levanta el stack:

docker compose up -d


Baja tu primer modelo (ej: Phi-3):

docker compose exec llm-server ollama pull phi3


(Revisa la secci√≥n de "Elegir tu Modelo" para m√°s opciones)

Carga tus documentos (Ingesta):

Coloca tus archivos (PDFs, .txt, .md) en una carpeta (ej. ./documentos-fuente).

Instala las dependencias para el script de ingesta (en tu m√°quina local, no en Docker):

pip install -r ingest/requirements.txt


Ejecuta el script (aseg√∫rate que vector-db expone el puerto 8001:8000 en tu docker-compose.yml para esto):

python3 ingest/ingest.py --source_dir ./documentos-fuente


Prueba el API

Una vez que tus documentos est√©n cargados, puedes hacer una consulta:

curl -X POST http://localhost:8080/v1/chat \
     -H "Content-Type: application/json" \
     -d '{
           "text": "¬øCu√°l es la filosof√≠a de Clean Architecture?"
         }'


Revisa tus dashboards:

Prometheus: http://localhost:9090

Grafana: http://localhost:3000

ü§ñ Eligiendo tu Modelo: Local vs. MVP vs. Producci√≥n

No todos los modelos son iguales. Elige el motor adecuado para tu carrera.

Etapa

Objetivo

Modelos Recomendados (Ejemplos)

Hardware M√≠nimo

Local (Dev)

Velocidad, fricci√≥n CERO.

phi3:mini, deepseek-coder:6.7b-instruct-q4

8GB RAM (CPU) / 8GB VRAM (GPU)

MVP (Team)

Validar valor, buen balance.

qwen:14b-chat-q5_K_M, deepseek-coder:33b-instruct-q4

16-32GB VRAM (GPU Mediana: T4, A10G)

Producci√≥n

Performance, razonamiento SOTA.

codestral:latest, qwen:72b-chat-q4_K_M

40GB+ VRAM (GPU Grande: A100, H100)

üí° Conceptos Clave: Pesos vs. Contexto

Para entender RAG, debes entender esta diferencia:

Pesos (Par√°metros): Es el Cerebro del LLM. Es el conocimiento permanente que el modelo "aprendi√≥" durante su entrenamiento (meses de c√≥mputo en supercomputadoras). Es est√°tico y pesado (gigas).

Analog√≠a: La educaci√≥n de un doctor en la facultad de medicina. Sabe qu√© es la diabetes, el c√°ncer y c√≥mo funciona el cuerpo humano.

Contexto (Ventana de Contexto): Es la Memoria RAM del LLM. Es la informaci√≥n temporal que le das en este instante. Es vol√°til y se limpia con cada nueva petici√≥n.

Analog√≠a: El expediente del paciente que le das al doctor.

Por qu√© RAG es una genialidad: No intentamos re-entrenar al doctor (modificar los pesos). Simplemente le pasamos el expediente correcto (el contexto de tu VectorDB) junto con la pregunta. El modelo usa su conocimiento (pesos) para analizar la informaci√≥n temporal (contexto) y darte una respuesta experta.

üìú Filosof√≠a de C√≥digo y Pruebas


Stateless API: El api-rag NO debe guardar estado. Todo estado persistente vive en vector-db o ollama-models.

Configuraci√≥n por Entorno: Cero hardcoding. URLs, nombres de modelos y tokens se manejan con variables de entorno (.env y docker-compose.yml).

L√≥gica Clara: El c√≥digo de prompting debe ser f√°cil de encontrar y modificar.

Pruebas

Un PR sin pruebas es un PR roto. Exigimos:

Unit Tests (Pytest): Prueba funciones puras (ej. chunkers, prompt_formatters). Mockea (simula) las llamadas a los servicios llm-server y vector-db.

Integration Tests: Prueba el flujo completo. Usa un docker-compose.override.yml para levantar una base de datos de prueba y un modelo ligero, y confirma que el endpoint /query devuelve una respuesta.

Pruebas de Calidad (RAG): (Avanzado) Mant√©n un "set dorado" de preguntas y respuestas esperadas para medir la calidad de la recuperaci√≥n (¬øEncontr√≥ el chunk correcto?) y la calidad de la generaci√≥n.

ü§ù C√≥mo Contribuir (¬°Eres bienvenido!)

Este es un proyecto open-source. Estamos felices de recibir forks, issues y Pull Requests.

El Proceso (Fork & Pull)

Habla primero: Si quieres agregar una feature grande, crea un Issue primero para discutirla.

Haz Fork del repositorio a tu propia cuenta de GitHub.

Crea una rama para tu feature: git checkout -b feat/mi-feature-genial

Programa (y a√±ade pruebas, ¬°por favor!).

Aseg√∫rate que el linter pase (ej. black . y ruff .).

Haz un Pull Request desde tu rama a la rama main de este repositorio.

Llena la plantilla de PR: Describe qu√© hace tu PR, por qu√© es necesario y c√≥mo podemos probarlo.

Revisa CONTRIBUTING.md para reglas m√°s detalladas.

üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Eres libre de usarlo, modificarlo y distribuirlo, incluso para uso comercial, siempre y cuando mantengas el aviso de licencia original.


![Python](https://img.shields.io/badge/python-v3.10+-blue.svg)
![Docker](https://img.shields.io/badge/docker-compose-ready-green.svg)
![License](https://img.shields.io/badge/license-MIT-yellow.svg)
![Architecture](https://img.shields.io/badge/architecture-clean-orange.svg)